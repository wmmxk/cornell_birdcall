{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bizon/anaconda3/envs/cornell_bird/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "from fastprogress import progress_bar\n",
    "import numpy as np\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prelim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = Path(\"/home/bizon/Data/cornell_birdcall\")\n",
    "INPUT_ROOT = project_dir/\"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "\n",
    "RAW_DATA = INPUT_ROOT / \"birdsong-recognition\"\n",
    "TRAIN_AUDIO_DIR = RAW_DATA / \"train_audio\"\n",
    "TEST_AUDIO_DIR = RAW_DATA / \"test_audio\"\n",
    "\n",
    "if not TEST_AUDIO_DIR.exists():\n",
    "    TEST_AUDIO_DIR = INPUT_ROOT / \"birdcall-check\" / \"test_audio\"\n",
    "    test = pd.read_csv(INPUT_ROOT / \"birdcall-check\" / \"test.csv\")\n",
    "else:\n",
    "    test = pd.read_csv(RAW_DATA / \"test.csv\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "BIRD_CODE = {\n",
    "    'aldfly': 0, 'ameavo': 1, 'amebit': 2, 'amecro': 3, 'amegfi': 4,\n",
    "    'amekes': 5, 'amepip': 6, 'amered': 7, 'amerob': 8, 'amewig': 9,\n",
    "    'amewoo': 10, 'amtspa': 11, 'annhum': 12, 'astfly': 13, 'baisan': 14,\n",
    "    'baleag': 15, 'balori': 16, 'banswa': 17, 'barswa': 18, 'bawwar': 19,\n",
    "    'belkin1': 20, 'belspa2': 21, 'bewwre': 22, 'bkbcuc': 23, 'bkbmag1': 24,\n",
    "    'bkbwar': 25, 'bkcchi': 26, 'bkchum': 27, 'bkhgro': 28, 'bkpwar': 29,\n",
    "    'bktspa': 30, 'blkpho': 31, 'blugrb1': 32, 'blujay': 33, 'bnhcow': 34,\n",
    "    'boboli': 35, 'bongul': 36, 'brdowl': 37, 'brebla': 38, 'brespa': 39,\n",
    "    'brncre': 40, 'brnthr': 41, 'brthum': 42, 'brwhaw': 43, 'btbwar': 44,\n",
    "    'btnwar': 45, 'btywar': 46, 'buffle': 47, 'buggna': 48, 'buhvir': 49,\n",
    "    'bulori': 50, 'bushti': 51, 'buwtea': 52, 'buwwar': 53, 'cacwre': 54,\n",
    "    'calgul': 55, 'calqua': 56, 'camwar': 57, 'cangoo': 58, 'canwar': 59,\n",
    "    'canwre': 60, 'carwre': 61, 'casfin': 62, 'caster1': 63, 'casvir': 64,\n",
    "    'cedwax': 65, 'chispa': 66, 'chiswi': 67, 'chswar': 68, 'chukar': 69,\n",
    "    'clanut': 70, 'cliswa': 71, 'comgol': 72, 'comgra': 73, 'comloo': 74,\n",
    "    'commer': 75, 'comnig': 76, 'comrav': 77, 'comred': 78, 'comter': 79,\n",
    "    'comyel': 80, 'coohaw': 81, 'coshum': 82, 'cowscj1': 83, 'daejun': 84,\n",
    "    'doccor': 85, 'dowwoo': 86, 'dusfly': 87, 'eargre': 88, 'easblu': 89,\n",
    "    'easkin': 90, 'easmea': 91, 'easpho': 92, 'eastow': 93, 'eawpew': 94,\n",
    "    'eucdov': 95, 'eursta': 96, 'evegro': 97, 'fiespa': 98, 'fiscro': 99,\n",
    "    'foxspa': 100, 'gadwal': 101, 'gcrfin': 102, 'gnttow': 103, 'gnwtea': 104,\n",
    "    'gockin': 105, 'gocspa': 106, 'goleag': 107, 'grbher3': 108, 'grcfly': 109,\n",
    "    'greegr': 110, 'greroa': 111, 'greyel': 112, 'grhowl': 113, 'grnher': 114,\n",
    "    'grtgra': 115, 'grycat': 116, 'gryfly': 117, 'haiwoo': 118, 'hamfly': 119,\n",
    "    'hergul': 120, 'herthr': 121, 'hoomer': 122, 'hoowar': 123, 'horgre': 124,\n",
    "    'horlar': 125, 'houfin': 126, 'houspa': 127, 'houwre': 128, 'indbun': 129,\n",
    "    'juntit1': 130, 'killde': 131, 'labwoo': 132, 'larspa': 133, 'lazbun': 134,\n",
    "    'leabit': 135, 'leafly': 136, 'leasan': 137, 'lecthr': 138, 'lesgol': 139,\n",
    "    'lesnig': 140, 'lesyel': 141, 'lewwoo': 142, 'linspa': 143, 'lobcur': 144,\n",
    "    'lobdow': 145, 'logshr': 146, 'lotduc': 147, 'louwat': 148, 'macwar': 149,\n",
    "    'magwar': 150, 'mallar3': 151, 'marwre': 152, 'merlin': 153, 'moublu': 154,\n",
    "    'mouchi': 155, 'moudov': 156, 'norcar': 157, 'norfli': 158, 'norhar2': 159,\n",
    "    'normoc': 160, 'norpar': 161, 'norpin': 162, 'norsho': 163, 'norwat': 164,\n",
    "    'nrwswa': 165, 'nutwoo': 166, 'olsfly': 167, 'orcwar': 168, 'osprey': 169,\n",
    "    'ovenbi1': 170, 'palwar': 171, 'pasfly': 172, 'pecsan': 173, 'perfal': 174,\n",
    "    'phaino': 175, 'pibgre': 176, 'pilwoo': 177, 'pingro': 178, 'pinjay': 179,\n",
    "    'pinsis': 180, 'pinwar': 181, 'plsvir': 182, 'prawar': 183, 'purfin': 184,\n",
    "    'pygnut': 185, 'rebmer': 186, 'rebnut': 187, 'rebsap': 188, 'rebwoo': 189,\n",
    "    'redcro': 190, 'redhea': 191, 'reevir1': 192, 'renpha': 193, 'reshaw': 194,\n",
    "    'rethaw': 195, 'rewbla': 196, 'ribgul': 197, 'rinduc': 198, 'robgro': 199,\n",
    "    'rocpig': 200, 'rocwre': 201, 'rthhum': 202, 'ruckin': 203, 'rudduc': 204,\n",
    "    'rufgro': 205, 'rufhum': 206, 'rusbla': 207, 'sagspa1': 208, 'sagthr': 209,\n",
    "    'savspa': 210, 'saypho': 211, 'scatan': 212, 'scoori': 213, 'semplo': 214,\n",
    "    'semsan': 215, 'sheowl': 216, 'shshaw': 217, 'snobun': 218, 'snogoo': 219,\n",
    "    'solsan': 220, 'sonspa': 221, 'sora': 222, 'sposan': 223, 'spotow': 224,\n",
    "    'stejay': 225, 'swahaw': 226, 'swaspa': 227, 'swathr': 228, 'treswa': 229,\n",
    "    'truswa': 230, 'tuftit': 231, 'tunswa': 232, 'veery': 233, 'vesspa': 234,\n",
    "    'vigswa': 235, 'warvir': 236, 'wesblu': 237, 'wesgre': 238, 'weskin': 239,\n",
    "    'wesmea': 240, 'wessan': 241, 'westan': 242, 'wewpew': 243, 'whbnut': 244,\n",
    "    'whcspa': 245, 'whfibi': 246, 'whtspa': 247, 'whtswi': 248, 'wilfly': 249,\n",
    "    'wilsni1': 250, 'wiltur': 251, 'winwre3': 252, 'wlswar': 253, 'wooduc': 254,\n",
    "    'wooscj2': 255, 'woothr': 256, 'y00475': 257, 'yebfly': 258, 'yebsap': 259,\n",
    "    'yehbla': 260, 'yelwar': 261, 'yerwar': 262, 'yetvir': 263\n",
    "}\n",
    "\n",
    "INV_BIRD_CODE = {v: k for k, v in BIRD_CODE.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Audio Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "ISC License\n",
    "Copyright (c) 2013--2017, librosa development team.\n",
    "\n",
    "Permission to use, copy, modify, and/or distribute this software for any purpose with or without fee is hereby granted, provided that the above copyright notice and this permission notice appear in all copies.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n",
    "'''\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "import torch.nn.functional as F\n",
    "class DFTBase(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Base class for DFT and IDFT matrix\"\"\"\n",
    "        super(DFTBase, self).__init__()\n",
    "\n",
    "    def dft_matrix(self, n):\n",
    "        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n",
    "        omega = np.exp(-2 * np.pi * 1j / n)\n",
    "        W = np.power(omega, x * y)\n",
    "        return W\n",
    "\n",
    "    def idft_matrix(self, n):\n",
    "        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n",
    "        omega = np.exp(2 * np.pi * 1j / n)\n",
    "        W = np.power(omega, x * y)\n",
    "        return W\n",
    "    \n",
    "    \n",
    "class STFT(DFTBase):\n",
    "    def __init__(self, n_fft=2048, hop_length=None, win_length=None, \n",
    "        window='hann', center=True, pad_mode='reflect', freeze_parameters=True):\n",
    "        \"\"\"Implementation of STFT with Conv1d. The function has the same output \n",
    "        of librosa.core.stft\n",
    "        \"\"\"\n",
    "        super(STFT, self).__init__()\n",
    "\n",
    "        assert pad_mode in ['constant', 'reflect']\n",
    "\n",
    "        self.n_fft = n_fft\n",
    "        self.center = center\n",
    "        self.pad_mode = pad_mode\n",
    "\n",
    "        # By default, use the entire frame\n",
    "        if win_length is None:\n",
    "            win_length = n_fft\n",
    "\n",
    "        # Set the default hop, if it's not already specified\n",
    "        if hop_length is None:\n",
    "            hop_length = int(win_length // 4)\n",
    "\n",
    "        fft_window = librosa.filters.get_window(window, win_length, fftbins=True)\n",
    "\n",
    "        # Pad the window out to n_fft size\n",
    "        fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
    "\n",
    "        # DFT & IDFT matrix\n",
    "        self.W = self.dft_matrix(n_fft)\n",
    "\n",
    "        out_channels = n_fft // 2 + 1\n",
    "\n",
    "        self.conv_real = nn.Conv1d(in_channels=1, out_channels=out_channels, \n",
    "            kernel_size=n_fft, stride=hop_length, padding=0, dilation=1, \n",
    "            groups=1, bias=False)\n",
    "\n",
    "        self.conv_imag = nn.Conv1d(in_channels=1, out_channels=out_channels, \n",
    "            kernel_size=n_fft, stride=hop_length, padding=0, dilation=1, \n",
    "            groups=1, bias=False)\n",
    "\n",
    "        self.conv_real.weight.data = torch.Tensor(\n",
    "            np.real(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n",
    "        # (n_fft // 2 + 1, 1, n_fft)\n",
    "\n",
    "        self.conv_imag.weight.data = torch.Tensor(\n",
    "            np.imag(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n",
    "        # (n_fft // 2 + 1, 1, n_fft)\n",
    "\n",
    "        if freeze_parameters:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"input: (batch_size, data_length)\n",
    "        Returns:\n",
    "          real: (batch_size, n_fft // 2 + 1, time_steps)\n",
    "          imag: (batch_size, n_fft // 2 + 1, time_steps)\n",
    "        \"\"\"\n",
    "\n",
    "        x = input[:, None, :]   # (batch_size, channels_num, data_length)\n",
    "\n",
    "        if self.center:\n",
    "            x = F.pad(x, pad=(self.n_fft // 2, self.n_fft // 2), mode=self.pad_mode)\n",
    "\n",
    "        real = self.conv_real(x)\n",
    "        imag = self.conv_imag(x)\n",
    "        # (batch_size, n_fft // 2 + 1, time_steps)\n",
    "\n",
    "        real = real[:, None, :, :].transpose(2, 3)\n",
    "        imag = imag[:, None, :, :].transpose(2, 3)\n",
    "        # (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
    "\n",
    "        return real, imag\n",
    "    \n",
    "    \n",
    "class Spectrogram(nn.Module):\n",
    "    def __init__(self, n_fft=2048, hop_length=None, win_length=None, \n",
    "        window='hann', center=True, pad_mode='reflect', power=2.0, \n",
    "        freeze_parameters=True):\n",
    "        \"\"\"Calculate spectrogram using pytorch. The STFT is implemented with \n",
    "        Conv1d. The function has the same output of librosa.core.stft\n",
    "        \"\"\"\n",
    "        super(Spectrogram, self).__init__()\n",
    "\n",
    "        self.power = power\n",
    "\n",
    "        self.stft = STFT(n_fft=n_fft, hop_length=hop_length, \n",
    "            win_length=win_length, window=window, center=center, \n",
    "            pad_mode=pad_mode, freeze_parameters=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"input: (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
    "        Returns:\n",
    "          spectrogram: (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
    "        \"\"\"\n",
    "\n",
    "        (real, imag) = self.stft.forward(input)\n",
    "        # (batch_size, n_fft // 2 + 1, time_steps)\n",
    "\n",
    "        spectrogram = real ** 2 + imag ** 2\n",
    "\n",
    "        if self.power == 2.0:\n",
    "            pass\n",
    "        else:\n",
    "            spectrogram = spectrogram ** (power / 2.0)\n",
    "\n",
    "        return spectrogram\n",
    "\n",
    "    \n",
    "class LogmelFilterBank(nn.Module):\n",
    "    def __init__(self, sr=32000, n_fft=2048, n_mels=64, fmin=50, fmax=14000, is_log=True, \n",
    "        ref=1.0, amin=1e-10, top_db=80.0, freeze_parameters=True):\n",
    "        \"\"\"Calculate logmel spectrogram using pytorch. The mel filter bank is \n",
    "        the pytorch implementation of as librosa.filters.mel \n",
    "        \"\"\"\n",
    "        super(LogmelFilterBank, self).__init__()\n",
    "\n",
    "        self.is_log = is_log\n",
    "        self.ref = ref\n",
    "        self.amin = amin\n",
    "        self.top_db = top_db\n",
    "\n",
    "        self.melW = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels,\n",
    "            fmin=fmin, fmax=fmax).T\n",
    "        # (n_fft // 2 + 1, mel_bins)\n",
    "\n",
    "        self.melW = nn.Parameter(torch.Tensor(self.melW))\n",
    "\n",
    "        if freeze_parameters:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"input: (batch_size, channels, time_steps)\n",
    "        \n",
    "        Output: (batch_size, time_steps, mel_bins)\n",
    "        \"\"\"\n",
    "\n",
    "        # Mel spectrogram\n",
    "        mel_spectrogram = torch.matmul(input, self.melW)\n",
    "\n",
    "        # Logmel spectrogram\n",
    "        if self.is_log:\n",
    "            output = self.power_to_db(mel_spectrogram)\n",
    "        else:\n",
    "            output = mel_spectrogram\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def power_to_db(self, input):\n",
    "        \"\"\"Power to db, this function is the pytorch implementation of \n",
    "        librosa.core.power_to_lb\n",
    "        \"\"\"\n",
    "        ref_value = self.ref\n",
    "        log_spec = 10.0 * torch.log10(torch.clamp(input, min=self.amin, max=np.inf))\n",
    "        log_spec -= 10.0 * np.log10(np.maximum(self.amin, ref_value))\n",
    "\n",
    "        if self.top_db is not None:\n",
    "            if self.top_db < 0:\n",
    "                raise ParameterError('top_db must be non-negative')\n",
    "            log_spec = torch.clamp(log_spec, min=log_spec.max().item() - self.top_db, max=np.inf)\n",
    "\n",
    "        return log_spec\n",
    "\n",
    "\n",
    "class DropStripes(nn.Module):\n",
    "    def __init__(self, dim, drop_width, stripes_num):\n",
    "        \"\"\"Drop stripes. \n",
    "        Args:\n",
    "          dim: int, dimension along which to drop\n",
    "          drop_width: int, maximum width of stripes to drop\n",
    "          stripes_num: int, how many stripes to drop\n",
    "        \"\"\"\n",
    "        super(DropStripes, self).__init__()\n",
    "\n",
    "        assert dim in [2, 3]    # dim 2: time; dim 3: frequency\n",
    "\n",
    "        self.dim = dim\n",
    "        self.drop_width = drop_width\n",
    "        self.stripes_num = stripes_num\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"input: (batch_size, channels, time_steps, freq_bins)\"\"\"\n",
    "\n",
    "        assert input.ndimension() == 4\n",
    "\n",
    "        if self.training is False:\n",
    "            return input\n",
    "\n",
    "        else:\n",
    "            batch_size = input.shape[0]\n",
    "            total_width = input.shape[self.dim]\n",
    "\n",
    "            for n in range(batch_size):\n",
    "                self.transform_slice(input[n], total_width)\n",
    "\n",
    "            return input\n",
    "\n",
    "\n",
    "    def transform_slice(self, e, total_width):\n",
    "        \"\"\"e: (channels, time_steps, freq_bins)\"\"\"\n",
    "\n",
    "        for _ in range(self.stripes_num):\n",
    "            distance = torch.randint(low=0, high=self.drop_width, size=(1,))[0]\n",
    "            bgn = torch.randint(low=0, high=total_width - distance, size=(1,))[0]\n",
    "\n",
    "            if self.dim == 2:\n",
    "                e[:, bgn : bgn + distance, :] = 0\n",
    "            elif self.dim == 3:\n",
    "                e[:, :, bgn : bgn + distance] = 0\n",
    "\n",
    "\n",
    "class SpecAugmentation(nn.Module):\n",
    "    def __init__(self, time_drop_width, time_stripes_num, freq_drop_width, \n",
    "        freq_stripes_num):\n",
    "        \"\"\"Spec augmetation. \n",
    "        [ref] Park, D.S., Chan, W., Zhang, Y., Chiu, C.C., Zoph, B., Cubuk, E.D. \n",
    "        and Le, Q.V., 2019. Specaugment: A simple data augmentation method \n",
    "        for automatic speech recognition. arXiv preprint arXiv:1904.08779.\n",
    "        Args:\n",
    "          time_drop_width: int\n",
    "          time_stripes_num: int\n",
    "          freq_drop_width: int\n",
    "          freq_stripes_num: int\n",
    "        \"\"\"\n",
    "\n",
    "        super(SpecAugmentation, self).__init__()\n",
    "\n",
    "        self.time_dropper = DropStripes(dim=2, drop_width=time_drop_width, \n",
    "            stripes_num=time_stripes_num)\n",
    "\n",
    "        self.freq_dropper = DropStripes(dim=3, drop_width=freq_drop_width, \n",
    "            stripes_num=freq_stripes_num)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.time_dropper(input)\n",
    "        x = self.freq_dropper(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 PANN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The MIT License\n",
    "  \n",
    "Copyright (c) 2018-2020 Qiuqiang Kong\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in\n",
    "all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    "THE SOFTWARE.\n",
    "'''\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "def interpolate(x: torch.Tensor, ratio: int):\n",
    "    \"\"\"Interpolate data in time domain. This is used to compensate the\n",
    "    resolution reduction in downsampling of a CNN.\n",
    "\n",
    "    Args:\n",
    "      x: (batch_size, time_steps, classes_num)\n",
    "      ratio: int, ratio to interpolate\n",
    "    Returns:\n",
    "      upsampled: (batch_size, time_steps * ratio, classes_num)\n",
    "    \"\"\"\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "\n",
    "def pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n",
    "    \"\"\"Pad framewise_output to the same length as input frames. The pad value\n",
    "    is the same as the value of the last frame.\n",
    "    Args:\n",
    "      framewise_output: (batch_size, frames_num, classes_num)\n",
    "      frames_num: int, number of frames to pad\n",
    "    Outputs:\n",
    "      output: (batch_size, frames_num, classes_num)\n",
    "    \"\"\"\n",
    "    pad = framewise_output[:, -1:, :].repeat(\n",
    "        1, frames_num - framewise_output.shape[1], 1)\n",
    "    \"\"\"tensor for padding\"\"\"\n",
    "\n",
    "    output = torch.cat((framewise_output, pad), dim=1)\n",
    "    \"\"\"(batch_size, frames_num, classes_num)\"\"\"\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            stride=(1, 1),\n",
    "            padding=(1, 1),\n",
    "            bias=False)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            stride=(1, 1),\n",
    "            padding=(1, 1),\n",
    "            bias=False)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_layer(self.conv2)\n",
    "        init_bn(self.bn1)\n",
    "        init_bn(self.bn2)\n",
    "\n",
    "    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n",
    "\n",
    "        x = input\n",
    "        x = F.relu_(self.bn1(self.conv1(x)))\n",
    "        x = F.relu_(self.bn2(self.conv2(x)))\n",
    "        if pool_type == 'max':\n",
    "            x = F.max_pool2d(x, kernel_size=pool_size)\n",
    "        elif pool_type == 'avg':\n",
    "            x = F.avg_pool2d(x, kernel_size=pool_size)\n",
    "        elif pool_type == 'avg+max':\n",
    "            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n",
    "            x2 = F.max_pool2d(x, kernel_size=pool_size)\n",
    "            x = x1 + x2\n",
    "        else:\n",
    "            raise Exception('Incorrect argument!')\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\",\n",
    "                 temperature=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.temperature = temperature\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.bn_att = nn.BatchNorm1d(out_features)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "        init_bn(self.bn_att)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "        \n",
    "class PANNsDense121Att(nn.Module):\n",
    "    def __init__(self, sample_rate: int, window_size: int, hop_size: int,\n",
    "                 mel_bins: int, fmin: int, fmax: int, classes_num: int, apply_aug: bool, top_db=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        self.interpolate_ratio = 32  # Downsampled ratio\n",
    "        self.apply_aug = apply_aug\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(\n",
    "            n_fft=window_size,\n",
    "            hop_length=hop_size,\n",
    "            win_length=window_size,\n",
    "            window=window,\n",
    "            center=center,\n",
    "            pad_mode=pad_mode,\n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(\n",
    "            sr=sample_rate,\n",
    "            n_fft=window_size,\n",
    "            n_mels=mel_bins,\n",
    "            fmin=fmin,\n",
    "            fmax=fmax,\n",
    "            ref=ref,\n",
    "            amin=amin,\n",
    "            top_db=top_db,\n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(\n",
    "            time_drop_width=64,\n",
    "            time_stripes_num=2,\n",
    "            freq_drop_width=8,\n",
    "            freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(mel_bins)\n",
    "\n",
    "        self.fc1 = nn.Linear(1024, 1024, bias=True)\n",
    "        self.att_block = AttBlock(1024, classes_num, activation='sigmoid')\n",
    "\n",
    "\n",
    "        self.densenet_features = models.densenet121(pretrained=False).features\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        \n",
    "    def cnn_feature_extractor(self, x):\n",
    "        x = self.densenet_features(x)\n",
    "        return x\n",
    "    \n",
    "    def preprocess(self, input_x, mixup_lambda=None):\n",
    "\n",
    "        x = self.spectrogram_extractor(input_x)  # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)  # (batch_size, 1, time_steps, mel_bins)\n",
    "\n",
    "        frames_num = x.shape[2]\n",
    "\n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "\n",
    "        if self.apply_aug:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        return x, frames_num\n",
    "        \n",
    "\n",
    "    def forward(self, input_data):\n",
    "        input_x, mixup_lambda = input_data\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "        b, c, s = input_x.shape\n",
    "        input_x = input_x.reshape(b*c, s)\n",
    "        x, frames_num = self.preprocess(input_x, mixup_lambda=mixup_lambda)\n",
    "        if mixup_lambda is not None:\n",
    "            b = (b*c)//2\n",
    "            c = 1\n",
    "        # Output shape (batch size, channels, time, frequency)\n",
    "        x = x.expand(x.shape[0], 3, x.shape[2], x.shape[3])\n",
    "        x = self.cnn_feature_extractor(x)\n",
    "        \n",
    "        # Aggregate in frequency axis\n",
    "        x = torch.mean(x, dim=3)\n",
    "\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
    "\n",
    "        # Get framewise output\n",
    "        framewise_output = interpolate(segmentwise_output,\n",
    "                                       self.interpolate_ratio)\n",
    "        framewise_output = pad_framewise_output(framewise_output, frames_num)\n",
    "        frame_shape =  framewise_output.shape\n",
    "        clip_shape = clipwise_output.shape\n",
    "        output_dict = {\n",
    "            'framewise_output': framewise_output.reshape(b, c, frame_shape[1],frame_shape[2]),\n",
    "            'clipwise_output': clipwise_output.reshape(b, c, clip_shape[1]),\n",
    "        }\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Model Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def get_model(ModelClass: object, config: dict, weights_path: str):\n",
    "    model = ModelClass(**config)\n",
    "    checkpoint = torch.load(weights_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = project_dir /\"model/1st_winning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_models = [\n",
    "    {\n",
    "        \"model_class\": PANNsDense121Att,\n",
    "        \"config\": {\n",
    "            \"sample_rate\": 32000,\n",
    "            \"window_size\": 1024,\n",
    "            \"hop_size\": 320,\n",
    "            \"mel_bins\": 64,\n",
    "            \"fmin\": 50,\n",
    "            \"fmax\": 14000,\n",
    "            \"classes_num\": 264,\n",
    "            \"apply_aug\": True,\n",
    "            \"top_db\": None\n",
    "        },\n",
    "        \"weights_path\": model_dir /\"final_sed_dense121_nomix_fold0_checkpoint_50_score0.7057.pt\",\n",
    "        \"clip_threshold\": 0.3,\n",
    "        \"threshold\": 0.3\n",
    "    },\n",
    "    {\n",
    "        \"model_class\": PANNsDense121Att,\n",
    "        \"config\": {\n",
    "            \"sample_rate\": 32000,\n",
    "            \"window_size\": 1024,\n",
    "            \"hop_size\": 320,\n",
    "            \"mel_bins\": 64,\n",
    "            \"fmin\": 50,\n",
    "            \"fmax\": 14000,\n",
    "            \"classes_num\": 264,\n",
    "            \"apply_aug\": True,\n",
    "            \"top_db\": None\n",
    "        },\n",
    "        \"weights_path\": model_dir /\"final_sed_dense121_nomix_fold1_checkpoint_48_score0.6943.pt\",\n",
    "        \"clip_threshold\": 0.3,\n",
    "        \"threshold\": 0.3\n",
    "    },\n",
    "    {\n",
    "        \"model_class\": PANNsDense121Att,\n",
    "        \"config\": {\n",
    "            \"sample_rate\": 32000,\n",
    "            \"window_size\": 1024,\n",
    "            \"hop_size\": 320,\n",
    "            \"mel_bins\": 64,\n",
    "            \"fmin\": 50,\n",
    "            \"fmax\": 14000,\n",
    "            \"classes_num\": 264,\n",
    "            \"apply_aug\": True,\n",
    "            \"top_db\": None\n",
    "        },\n",
    "        \"weights_path\": model_dir /\"final_sed_dense121_nomix_fold2_augd_checkpoint_50_score0.6666.pt\",\n",
    "        \"clip_threshold\": 0.3,\n",
    "        \"threshold\": 0.3\n",
    "    },\n",
    "    {\n",
    "        \"model_class\": PANNsDense121Att,\n",
    "        \"config\": {\n",
    "            \"sample_rate\": 32000,\n",
    "            \"window_size\": 1024,\n",
    "            \"hop_size\": 320,\n",
    "            \"mel_bins\": 64,\n",
    "            \"fmin\": 50,\n",
    "            \"fmax\": 14000,\n",
    "            \"classes_num\": 264,\n",
    "            \"apply_aug\": True,\n",
    "            \"top_db\": None\n",
    "        },\n",
    "        \"weights_path\": model_dir /\"final_sed_dense121_nomix_fold3_augd_checkpoint_50_score0.6713.pt\",\n",
    "        \"clip_threshold\": 0.3,\n",
    "        \"threshold\": 0.3\n",
    "    },\n",
    "    {\n",
    "        \"model_class\": PANNsDense121Att,\n",
    "        \"config\": {\n",
    "            \"sample_rate\": 32000,\n",
    "            \"window_size\": 1024,\n",
    "            \"hop_size\": 320,\n",
    "            \"mel_bins\": 64,\n",
    "            \"fmin\": 50,\n",
    "            \"fmax\": 14000,\n",
    "            \"classes_num\": 264,\n",
    "            \"apply_aug\": True,\n",
    "            \"top_db\": None\n",
    "        },\n",
    "        \"weights_path\": model_dir /\"final_5fold_sed_dense121_nomix_fold0_checkpoint_50_score0.7219.pt\",\n",
    "        \"clip_threshold\": 0.3,\n",
    "        \"threshold\": 0.3\n",
    "    },\n",
    "    {\n",
    "        \"model_class\": PANNsDense121Att,\n",
    "        \"config\": {\n",
    "            \"sample_rate\": 32000,\n",
    "            \"window_size\": 1024,\n",
    "            \"hop_size\": 320,\n",
    "            \"mel_bins\": 64,\n",
    "            \"fmin\": 50,\n",
    "            \"fmax\": 14000,\n",
    "            \"classes_num\": 264,\n",
    "            \"apply_aug\": True,\n",
    "            \"top_db\": None\n",
    "        },\n",
    "        \"weights_path\": model_dir /\"final_5fold_sed_dense121_nomix_fold1_checkpoint_44_score0.7645.pt\",\n",
    "        \"clip_threshold\": 0.3,\n",
    "        \"threshold\": 0.3\n",
    "    },\n",
    "    {\n",
    "        \"model_class\": PANNsDense121Att,\n",
    "        \"config\": {\n",
    "            \"sample_rate\": 32000,\n",
    "            \"window_size\": 1024,\n",
    "            \"hop_size\": 320,\n",
    "            \"mel_bins\": 64,\n",
    "            \"fmin\": 50,\n",
    "            \"fmax\": 14000,\n",
    "            \"classes_num\": 264,\n",
    "            \"apply_aug\": True,\n",
    "            \"top_db\": None\n",
    "        },\n",
    "        \"weights_path\": model_dir /\"final_5fold_sed_dense121_nomix_fold2_checkpoint_50_score0.7737.pt\",\n",
    "        \"clip_threshold\": 0.3,\n",
    "        \"threshold\": 0.3\n",
    "    },\n",
    "    {\n",
    "        \"model_class\": PANNsDense121Att,\n",
    "        \"config\": {\n",
    "            \"sample_rate\": 32000,\n",
    "            \"window_size\": 1024,\n",
    "            \"hop_size\": 320,\n",
    "            \"mel_bins\": 64,\n",
    "            \"fmin\": 50,\n",
    "            \"fmax\": 14000,\n",
    "            \"classes_num\": 264,\n",
    "            \"apply_aug\": True,\n",
    "            \"top_db\": None\n",
    "        },\n",
    "        \"weights_path\": model_dir /\"final_5fold_sed_dense121_nomix_fold3_checkpoint_48_score0.7746.pt\",\n",
    "        \"clip_threshold\": 0.3,\n",
    "        \"threshold\": 0.3\n",
    "    },\n",
    "    {\n",
    "        \"model_class\": PANNsDense121Att,\n",
    "        \"config\": {\n",
    "            \"sample_rate\": 32000,\n",
    "            \"window_size\": 1024,\n",
    "            \"hop_size\": 320,\n",
    "            \"mel_bins\": 64,\n",
    "            \"fmin\": 50,\n",
    "            \"fmax\": 14000,\n",
    "            \"classes_num\": 264,\n",
    "            \"apply_aug\": True,\n",
    "            \"top_db\": None\n",
    "        },\n",
    "        \"weights_path\": model_dir /\"final_5fold_sed_dense121_nomix_fold4_checkpoint_50_score0.7728.pt\",\n",
    "        \"clip_threshold\": 0.3,\n",
    "        \"threshold\": 0.3\n",
    "    },\n",
    "    {\n",
    "        \"model_class\": PANNsDense121Att,\n",
    "        \"config\": {\n",
    "            \"sample_rate\": 32000,\n",
    "            \"window_size\": 1024,\n",
    "            \"hop_size\": 320,\n",
    "            \"mel_bins\": 64,\n",
    "            \"fmin\": 50,\n",
    "            \"fmax\": 14000,\n",
    "            \"classes_num\": 264,\n",
    "            \"apply_aug\": True,\n",
    "            \"top_db\": None\n",
    "        },\n",
    "        \"weights_path\": model_dir /\"final_sed_dense121_mix_fold0_2_checkpoint_50_score0.6842.pt\",\n",
    "        \"clip_threshold\": 0.3,\n",
    "        \"threshold\": 0.3\n",
    "    },\n",
    "    {\n",
    "        \"model_class\": PANNsDense121Att,\n",
    "        \"config\": {\n",
    "            \"sample_rate\": 32000,\n",
    "            \"window_size\": 1024,\n",
    "            \"hop_size\": 320,\n",
    "            \"mel_bins\": 64,\n",
    "            \"fmin\": 50,\n",
    "            \"fmax\": 14000,\n",
    "            \"classes_num\": 264,\n",
    "            \"apply_aug\": True,\n",
    "            \"top_db\": None\n",
    "        },\n",
    "        \"weights_path\": model_dir /\"final_sed_dense121_mix_fold1_2_checkpoint_50_score0.6629.pt\",\n",
    "        \"clip_threshold\": 0.3,\n",
    "        \"threshold\": 0.3\n",
    "    },\n",
    "    {\n",
    "        \"model_class\": PANNsDense121Att,\n",
    "        \"config\": {\n",
    "            \"sample_rate\": 32000,\n",
    "            \"window_size\": 1024,\n",
    "            \"hop_size\": 320,\n",
    "            \"mel_bins\": 64,\n",
    "            \"fmin\": 50,\n",
    "            \"fmax\": 14000,\n",
    "            \"classes_num\": 264,\n",
    "            \"apply_aug\": True,\n",
    "            \"top_db\": None\n",
    "        },\n",
    "        \"weights_path\": model_dir /\"final_sed_dense121_mix_fold2_2_checkpoint_50_score0.6884.pt\",\n",
    "        \"clip_threshold\": 0.3,\n",
    "        \"threshold\": 0.3\n",
    "    },\n",
    "    {\n",
    "        \"model_class\": PANNsDense121Att,\n",
    "        \"config\": {\n",
    "            \"sample_rate\": 32000,\n",
    "            \"window_size\": 1024,\n",
    "            \"hop_size\": 320,\n",
    "            \"mel_bins\": 64,\n",
    "            \"fmin\": 50,\n",
    "            \"fmax\": 14000,\n",
    "            \"classes_num\": 264,\n",
    "            \"apply_aug\": True,\n",
    "            \"top_db\": None\n",
    "        },\n",
    "        \"weights_path\": model_dir /\"final_sed_dense121_mix_fold3_2_checkpoint_50_score0.6870.pt\",\n",
    "        \"clip_threshold\": 0.3,\n",
    "        \"threshold\": 0.3\n",
    "    }\n",
    "]\n",
    "PERIOD = 30\n",
    "SR = 32000\n",
    "vote_lim = 4\n",
    "TTA = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "for lm in list_of_models:\n",
    "    lm[\"model\"] = get_model(lm[\"model_class\"], lm[\"config\"], lm[\"weights_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list_of_models[0][\"model\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def prediction_for_clip(test_df: pd.DataFrame,\n",
    "                        clip: np.ndarray, \n",
    "                        model,\n",
    "                        threshold,\n",
    "                       clip_threshold):\n",
    "\n",
    "    audios = []\n",
    "    y = clip.astype(np.float32)\n",
    "    len_y = len(y)\n",
    "    start = 0\n",
    "    end = PERIOD * SR\n",
    "    while True:\n",
    "        y_batch = y[start:end].astype(np.float32)\n",
    "        if len(y_batch) != PERIOD * SR:\n",
    "            y_pad = np.zeros(PERIOD * SR, dtype=np.float32)\n",
    "            y_pad[:len(y_batch)] = y_batch\n",
    "            audios.append(y_pad)\n",
    "            break\n",
    "        start = end\n",
    "        end += PERIOD * SR\n",
    "        audios.append(y_batch)\n",
    "        \n",
    "    array = np.asarray(audios)\n",
    "    tensors = torch.from_numpy(array)\n",
    "    \n",
    "    model.eval()\n",
    "    estimated_event_list = []\n",
    "    global_time = 0.0\n",
    "    site = test_df[\"site\"].values[0]\n",
    "    audio_id = test_df[\"audio_id\"].values[0]\n",
    "    for image in tensors:\n",
    "        image = image.unsqueeze(0).unsqueeze(0)\n",
    "        image = image.expand(image.shape[0], TTA, image.shape[2])\n",
    "        image = image.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prediction = model((image, None))\n",
    "            framewise_outputs = prediction[\"framewise_output\"].detach(\n",
    "                ).cpu().numpy()[0].mean(axis=0)\n",
    "            clipwise_outputs = prediction[\"clipwise_output\"].detach(\n",
    "                ).cpu().numpy()[0].mean(axis=0)\n",
    "                \n",
    "        thresholded = framewise_outputs >= threshold\n",
    "        \n",
    "        clip_thresholded = clipwise_outputs >= clip_threshold\n",
    "        clip_indices = np.argwhere(clip_thresholded).reshape(-1)\n",
    "        clip_codes = []\n",
    "        for ci in clip_indices:\n",
    "            clip_codes.append(INV_BIRD_CODE[ci])\n",
    "            \n",
    "        for target_idx in range(thresholded.shape[1]):\n",
    "            if thresholded[:, target_idx].mean() == 0:\n",
    "                pass\n",
    "            else:\n",
    "                detected = np.argwhere(thresholded[:, target_idx]).reshape(-1)\n",
    "                head_idx = 0\n",
    "                tail_idx = 0\n",
    "                while True:\n",
    "                    if (tail_idx + 1 == len(detected)) or (\n",
    "                            detected[tail_idx + 1] - \n",
    "                            detected[tail_idx] != 1):\n",
    "                        onset = 0.01 * detected[\n",
    "                            head_idx] + global_time\n",
    "                        offset = 0.01 * detected[\n",
    "                            tail_idx] + global_time\n",
    "                        onset_idx = detected[head_idx]\n",
    "                        offset_idx = detected[tail_idx]\n",
    "                        max_confidence = framewise_outputs[\n",
    "                            onset_idx:offset_idx, target_idx].max()\n",
    "                        mean_confidence = framewise_outputs[\n",
    "                            onset_idx:offset_idx, target_idx].mean()\n",
    "                        if INV_BIRD_CODE[target_idx] in clip_codes:\n",
    "                            estimated_event = {\n",
    "                                \"site\": site,\n",
    "                                \"audio_id\": audio_id,\n",
    "                                \"ebird_code\": INV_BIRD_CODE[target_idx],\n",
    "                                \"clip_codes\": clip_codes,\n",
    "                                \"onset\": onset,\n",
    "                                \"offset\": offset,\n",
    "                                \"max_confidence\": max_confidence,\n",
    "                                \"mean_confidence\": mean_confidence\n",
    "                            }\n",
    "                            estimated_event_list.append(estimated_event)\n",
    "                        head_idx = tail_idx + 1\n",
    "                        tail_idx = tail_idx + 1\n",
    "                        if head_idx >= len(detected):\n",
    "                            break\n",
    "                    else:\n",
    "                        tail_idx += 1\n",
    "        global_time += PERIOD\n",
    "        \n",
    "    prediction_df = pd.DataFrame(estimated_event_list)\n",
    "    return prediction_df\n",
    "\n",
    "def prediction(test_df: pd.DataFrame,\n",
    "               test_audio: Path,\n",
    "               list_of_model_details):\n",
    "    unique_audio_id = test_df.audio_id.unique()\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    prediction_dfs_dict = defaultdict(list)\n",
    "    for audio_id in progress_bar(unique_audio_id):\n",
    "        clip, _ = librosa.load(test_audio / (audio_id + \".mp3\"),\n",
    "                               sr=SR,\n",
    "                               mono=True,\n",
    "                               res_type=\"kaiser_fast\")\n",
    "        \n",
    "        test_df_for_audio_id = test_df.query(\n",
    "            f\"audio_id == '{audio_id}'\").reset_index(drop=True)\n",
    "        for i, model_details in enumerate(list_of_model_details):\n",
    "            prediction_df = prediction_for_clip(test_df_for_audio_id,\n",
    "                                                clip=clip,\n",
    "                                                model=model_details[\"model\"],\n",
    "                                                threshold=model_details[\"threshold\"],\n",
    "                                               clip_threshold=model_details[\"clip_threshold\"])\n",
    "\n",
    "            prediction_dfs_dict[i].append(prediction_df)\n",
    "    list_of_prediction_df = []\n",
    "    for key, prediction_dfs in prediction_dfs_dict.items():\n",
    "        prediction_df = pd.concat(prediction_dfs, axis=0, sort=False).reset_index(drop=True)\n",
    "        list_of_prediction_df.append(prediction_df)\n",
    "    return list_of_prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_kg_hide-output": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='15' class='' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [15/15 00:43<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prediction_df = prediction(test_df=test,\n",
    "                           test_audio=TEST_AUDIO_DIR,\n",
    "                           list_of_model_details=list_of_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(470, 8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_prediction_df[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Post Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def get_post_post_process_predictions(prediction_df):\n",
    "    labels = {}\n",
    "\n",
    "    for audio_id, sub_df in progress_bar(prediction_df.groupby(\"audio_id\")):\n",
    "        events = sub_df[[\"ebird_code\", \"onset\", \"offset\", \"max_confidence\", \"site\"]].values\n",
    "        n_events = len(events)\n",
    "\n",
    "        site = events[0][4]\n",
    "        for i in range(n_events):\n",
    "            event = events[i][0]\n",
    "            onset = events[i][1]\n",
    "            offset = events[i][2]\n",
    "            \n",
    "            start_section = int((onset // 5) * 5) + 5\n",
    "            end_section = int((offset // 5) * 5) + 5\n",
    "            cur_section = start_section\n",
    "\n",
    "            row_id = f\"{site}_{audio_id}_{start_section}\"\n",
    "            if labels.get(row_id) is not None:\n",
    "                labels[row_id].add(event)\n",
    "            else:\n",
    "                labels[row_id] = set()\n",
    "                labels[row_id].add(event)\n",
    "\n",
    "            while cur_section != end_section:\n",
    "                cur_section += 5\n",
    "                row_id = f\"{site}_{audio_id}_{cur_section}\"\n",
    "                if labels.get(row_id) is not None:\n",
    "                    labels[row_id].add(event)\n",
    "                else:\n",
    "                    labels[row_id] = set()\n",
    "                    labels[row_id].add(event)\n",
    "\n",
    "\n",
    "    for key in labels:\n",
    "        labels[key] = \" \".join(sorted(list(labels[key])))\n",
    "\n",
    "\n",
    "    row_ids = list(labels.keys())\n",
    "    birds = list(labels.values())\n",
    "    post_processed = pd.DataFrame({\n",
    "        \"row_id\": row_ids,\n",
    "        \"birds\": birds\n",
    "    })\n",
    "    return post_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='15' class='' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [15/15 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='14' class='' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [14/14 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='15' class='' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [15/15 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='15' class='' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [15/15 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='15' class='' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [15/15 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='14' class='' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [14/14 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='15' class='' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [15/15 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='15' class='' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [15/15 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='15' class='' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [15/15 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='15' class='' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [15/15 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='14' class='' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [14/14 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='15' class='' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [15/15 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='15' class='' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [15/15 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_row_id = test[[\"row_id\"]]\n",
    "list_of_submissions = []\n",
    "for prediction_df in list_of_prediction_df:\n",
    "    post_processed = get_post_post_process_predictions(prediction_df)\n",
    "    submission = post_processed.fillna(\"nocall\")\n",
    "    submission = submission.set_index('row_id')\n",
    "    list_of_submissions.append(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all_of_row_ids = []\n",
    "for sub_x in list_of_submissions:\n",
    "    list_all_of_row_ids+= list(sub_x.index.values)\n",
    "list_all_of_row_ids = list(set(list_all_of_row_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission = []\n",
    "for row_id in list_all_of_row_ids:\n",
    "    birds = []\n",
    "    for sub in list_of_submissions:\n",
    "        if row_id in sub.index:\n",
    "            birds.extend(sub.loc[row_id].birds.split(\" \"))\n",
    "    birds = [x for x in birds if \"nocall\" != x and \"\" != x]\n",
    "    count_birds = Counter(birds)\n",
    "    final_birds = []\n",
    "    for key, value in count_birds.items():\n",
    "        if value >= vote_lim:\n",
    "            final_birds.append(key)\n",
    "    if len(final_birds)>0:\n",
    "        row_data = {\n",
    "            \"row_id\": row_id,\n",
    "            \"birds\": \" \".join(sorted(final_birds))\n",
    "        }\n",
    "    else:\n",
    "        row_data = {\n",
    "            \"row_id\": row_id,\n",
    "            \"birds\": \"nocall\"\n",
    "        }\n",
    "    final_submission.append(row_data)\n",
    "    \n",
    "site_3_data = defaultdict(list)\n",
    "for row in final_submission:\n",
    "    if \"site_3\" in row[\"row_id\"]:\n",
    "        final_row_id = \"_\".join(row[\"row_id\"].split(\"_\")[0:-1])\n",
    "        birds = row[\"birds\"].split(\" \")\n",
    "        birds = [x for x in birds if \"nocall\" != x and \"\" != x]\n",
    "        site_3_data[final_row_id].extend(birds)\n",
    "        \n",
    "for key, value in site_3_data.items():\n",
    "    count_birds = Counter(value)\n",
    "    final_birds = []\n",
    "    for k, v in count_birds.items():\n",
    "        if v >= vote_lim:\n",
    "            final_birds.append(k)\n",
    "    if len(final_birds)>0:\n",
    "        row_data = {\n",
    "            \"row_id\": key,\n",
    "            \"birds\": \" \".join(sorted(final_birds))\n",
    "        }\n",
    "    else:\n",
    "        row_data = {\n",
    "            \"row_id\": key,\n",
    "            \"birds\": \"nocall\"\n",
    "        }\n",
    "    final_submission.append(row_data)\n",
    "\n",
    "final_submission = pd.DataFrame(final_submission)\n",
    "final_submission = all_row_id.merge(final_submission, on=\"row_id\", how=\"left\")\n",
    "final_submission = final_submission.fillna(\"nocall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>birds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>site_1_41e6fe6504a34bf6846938ba78d13df1_5</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>site_1_41e6fe6504a34bf6846938ba78d13df1_10</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>site_1_41e6fe6504a34bf6846938ba78d13df1_15</td>\n",
       "      <td>aldfly moudov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>site_1_41e6fe6504a34bf6846938ba78d13df1_20</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>site_1_41e6fe6504a34bf6846938ba78d13df1_25</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>site_1_cce64fffafed40f2b2f3d3413ec1c4c2_5</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>site_1_cce64fffafed40f2b2f3d3413ec1c4c2_10</td>\n",
       "      <td>nocall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>site_1_cce64fffafed40f2b2f3d3413ec1c4c2_15</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>site_1_cce64fffafed40f2b2f3d3413ec1c4c2_20</td>\n",
       "      <td>nocall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>site_1_cce64fffafed40f2b2f3d3413ec1c4c2_25</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>site_1_cce64fffafed40f2b2f3d3413ec1c4c2_30</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>site_1_cce64fffafed40f2b2f3d3413ec1c4c2_35</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>site_1_99af324c881246949408c0b1ae54271f_5</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>site_1_99af324c881246949408c0b1ae54271f_10</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>site_1_99af324c881246949408c0b1ae54271f_15</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>site_1_99af324c881246949408c0b1ae54271f_20</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>site_1_99af324c881246949408c0b1ae54271f_25</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>site_1_99af324c881246949408c0b1ae54271f_30</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>site_1_99af324c881246949408c0b1ae54271f_35</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>site_1_6ab74e177aa149468a39ca10beed6222_5</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>site_1_6ab74e177aa149468a39ca10beed6222_10</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>site_1_6ab74e177aa149468a39ca10beed6222_15</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>site_1_6ab74e177aa149468a39ca10beed6222_20</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>site_1_6ab74e177aa149468a39ca10beed6222_25</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>site_1_6ab74e177aa149468a39ca10beed6222_30</td>\n",
       "      <td>nocall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>site_1_b2fd3f01e9284293a1e33f9c811a2ed6_5</td>\n",
       "      <td>nocall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>site_1_b2fd3f01e9284293a1e33f9c811a2ed6_10</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>site_1_b2fd3f01e9284293a1e33f9c811a2ed6_15</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>site_1_b2fd3f01e9284293a1e33f9c811a2ed6_20</td>\n",
       "      <td>nocall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>site_1_b2fd3f01e9284293a1e33f9c811a2ed6_25</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>site_1_b2fd3f01e9284293a1e33f9c811a2ed6_30</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>site_1_b2fd3f01e9284293a1e33f9c811a2ed6_35</td>\n",
       "      <td>aldfly eastow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>site_2_de62b37ebba749d2abf29d4a493ea5d4_5</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>site_2_8680a8dd845d40f296246dbed0d37394_5</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>site_2_8680a8dd845d40f296246dbed0d37394_10</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>site_2_8680a8dd845d40f296246dbed0d37394_15</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>site_2_8680a8dd845d40f296246dbed0d37394_20</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>site_2_8680a8dd845d40f296246dbed0d37394_25</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>site_2_8680a8dd845d40f296246dbed0d37394_30</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>site_2_8680a8dd845d40f296246dbed0d37394_35</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>site_2_8680a8dd845d40f296246dbed0d37394_40</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>site_2_8680a8dd845d40f296246dbed0d37394_45</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>site_2_940d546e5eb745c9a74bce3f35efa1f9_5</td>\n",
       "      <td>aldfly comyel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>site_2_940d546e5eb745c9a74bce3f35efa1f9_10</td>\n",
       "      <td>aldfly comyel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>site_2_940d546e5eb745c9a74bce3f35efa1f9_15</td>\n",
       "      <td>aldfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>site_2_940d546e5eb745c9a74bce3f35efa1f9_20</td>\n",
       "      <td>nocall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>site_2_940d546e5eb745c9a74bce3f35efa1f9_25</td>\n",
       "      <td>comyel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>site_2_940d546e5eb745c9a74bce3f35efa1f9_30</td>\n",
       "      <td>comyel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>site_2_940d546e5eb745c9a74bce3f35efa1f9_35</td>\n",
       "      <td>comyel houwre sonspa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>site_2_940d546e5eb745c9a74bce3f35efa1f9_40</td>\n",
       "      <td>comyel houwre reevir1 sonspa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        row_id                         birds\n",
       "0    site_1_41e6fe6504a34bf6846938ba78d13df1_5                        aldfly\n",
       "1   site_1_41e6fe6504a34bf6846938ba78d13df1_10                        aldfly\n",
       "2   site_1_41e6fe6504a34bf6846938ba78d13df1_15                 aldfly moudov\n",
       "3   site_1_41e6fe6504a34bf6846938ba78d13df1_20                        aldfly\n",
       "4   site_1_41e6fe6504a34bf6846938ba78d13df1_25                        aldfly\n",
       "5    site_1_cce64fffafed40f2b2f3d3413ec1c4c2_5                        aldfly\n",
       "6   site_1_cce64fffafed40f2b2f3d3413ec1c4c2_10                        nocall\n",
       "7   site_1_cce64fffafed40f2b2f3d3413ec1c4c2_15                        aldfly\n",
       "8   site_1_cce64fffafed40f2b2f3d3413ec1c4c2_20                        nocall\n",
       "9   site_1_cce64fffafed40f2b2f3d3413ec1c4c2_25                        aldfly\n",
       "10  site_1_cce64fffafed40f2b2f3d3413ec1c4c2_30                        aldfly\n",
       "11  site_1_cce64fffafed40f2b2f3d3413ec1c4c2_35                        aldfly\n",
       "12   site_1_99af324c881246949408c0b1ae54271f_5                        aldfly\n",
       "13  site_1_99af324c881246949408c0b1ae54271f_10                        aldfly\n",
       "14  site_1_99af324c881246949408c0b1ae54271f_15                        aldfly\n",
       "15  site_1_99af324c881246949408c0b1ae54271f_20                        aldfly\n",
       "16  site_1_99af324c881246949408c0b1ae54271f_25                        aldfly\n",
       "17  site_1_99af324c881246949408c0b1ae54271f_30                        aldfly\n",
       "18  site_1_99af324c881246949408c0b1ae54271f_35                        aldfly\n",
       "19   site_1_6ab74e177aa149468a39ca10beed6222_5                        aldfly\n",
       "20  site_1_6ab74e177aa149468a39ca10beed6222_10                        aldfly\n",
       "21  site_1_6ab74e177aa149468a39ca10beed6222_15                        aldfly\n",
       "22  site_1_6ab74e177aa149468a39ca10beed6222_20                        aldfly\n",
       "23  site_1_6ab74e177aa149468a39ca10beed6222_25                        aldfly\n",
       "24  site_1_6ab74e177aa149468a39ca10beed6222_30                        nocall\n",
       "25   site_1_b2fd3f01e9284293a1e33f9c811a2ed6_5                        nocall\n",
       "26  site_1_b2fd3f01e9284293a1e33f9c811a2ed6_10                        aldfly\n",
       "27  site_1_b2fd3f01e9284293a1e33f9c811a2ed6_15                        aldfly\n",
       "28  site_1_b2fd3f01e9284293a1e33f9c811a2ed6_20                        nocall\n",
       "29  site_1_b2fd3f01e9284293a1e33f9c811a2ed6_25                        aldfly\n",
       "30  site_1_b2fd3f01e9284293a1e33f9c811a2ed6_30                        aldfly\n",
       "31  site_1_b2fd3f01e9284293a1e33f9c811a2ed6_35                 aldfly eastow\n",
       "32   site_2_de62b37ebba749d2abf29d4a493ea5d4_5                        aldfly\n",
       "33   site_2_8680a8dd845d40f296246dbed0d37394_5                        aldfly\n",
       "34  site_2_8680a8dd845d40f296246dbed0d37394_10                        aldfly\n",
       "35  site_2_8680a8dd845d40f296246dbed0d37394_15                        aldfly\n",
       "36  site_2_8680a8dd845d40f296246dbed0d37394_20                        aldfly\n",
       "37  site_2_8680a8dd845d40f296246dbed0d37394_25                        aldfly\n",
       "38  site_2_8680a8dd845d40f296246dbed0d37394_30                        aldfly\n",
       "39  site_2_8680a8dd845d40f296246dbed0d37394_35                        aldfly\n",
       "40  site_2_8680a8dd845d40f296246dbed0d37394_40                        aldfly\n",
       "41  site_2_8680a8dd845d40f296246dbed0d37394_45                        aldfly\n",
       "42   site_2_940d546e5eb745c9a74bce3f35efa1f9_5                 aldfly comyel\n",
       "43  site_2_940d546e5eb745c9a74bce3f35efa1f9_10                 aldfly comyel\n",
       "44  site_2_940d546e5eb745c9a74bce3f35efa1f9_15                        aldfly\n",
       "45  site_2_940d546e5eb745c9a74bce3f35efa1f9_20                        nocall\n",
       "46  site_2_940d546e5eb745c9a74bce3f35efa1f9_25                        comyel\n",
       "47  site_2_940d546e5eb745c9a74bce3f35efa1f9_30                        comyel\n",
       "48  site_2_940d546e5eb745c9a74bce3f35efa1f9_35          comyel houwre sonspa\n",
       "49  site_2_940d546e5eb745c9a74bce3f35efa1f9_40  comyel houwre reevir1 sonspa"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_submission.to_csv(\"submission.csv\", index=False)\n",
    "final_submission.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
